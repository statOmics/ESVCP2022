---
title: "2. Rank Tests"
author: "Lieven Clement"
date: "statOmics, Ghent University (https://statomics.github.io)"
---

```{r setup, include=FALSE, cache=FALSE}
knitr::opts_chunk$set(
  include = TRUE, comment = NA, echo = TRUE,
  message = FALSE, warning = FALSE, cache = TRUE
)
library(tidyverse)
```


# Introduction

Inference was only correct if distributional assumptions were satisfied

- e.g Normal distribution
- equal variance

-  The $p$-value: $\text{P}_0\left[ \vert T\vert \geq \vert t \vert \right]$.

	- Calculated using the null distribution of $T$ that we derived under the assumptions
	- In correct if assumptions are violated

-  $95\%$ CI also builds upon these assumptions. If they are invalid then the intervals will not contain the population parameter with 95% probability.

- Asymptotic theory is more difficult to place: the $t$-test is asymptotically non-parametric because for very large samples the distributional assumptions of normality are no longer important.

- If assumptions hold the parametric approach

	- more efficient: larger power with same sample size + smaller CI.
	- more flexible: easier to analyse data with complex designs

---

## Cholesterol example

- Cholesterol concentration in blood measured for
  - 5 patients (group=1) two days upon a stroke
  - 5 healthy subject (groep=2).

- Is cholesterol concentration of hart patients and healthy subjects on average different?

```{r}
chol <- read_tsv("https://raw.githubusercontent.com/GTPB/PSLS20/master/data/chol.txt")
chol$group <- as.factor(chol$group)
nGroups <- table(chol$group)
n <- sum(nGroups)
chol
```

---


```{r, echo=FALSE, fig.align='center'}
chol %>% ggplot(aes(x = group, y = cholest)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(position = "jitter")

chol %>% ggplot(aes(sample = cholest)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~group)
```

- Possibly outliers
- Difficult to assess distributional assumptions when only 5 observations are available.

## Permutation tests

If the data are not normally distributed or if we do not want to make distributional assumptions we can resort to a permutation test. 

$$
H_0: f_1(x)=f_2(x)
$$

- Problem if we can reject the null hypothesis that their is an effect of the treatment, how can we interpret how the difference between both distributions? 

- In order enable an useful interpretation if we use an permutation t-test, we have to make additional assumptions: 

- location shift model: we assume that the distributions have the same shape but that the treatment can only induce a shift in the location of the distribution:

$$ 
H_0: f_1(x)=f_2(x) \text{ vs } H_1: \{\forall y: f_1(y)=f_2(y-\Delta)\}
$$
with 
$$
\Delta = \mu_1 - \mu_2
$$

- Under this assumption we can again interpret a rejection of $H_0$ in terms of a difference in means. 

- However, we would have to assess this assumption. 

- Instead we will resort to rank tests, which will allow to further relax this assumption! 

## Rank Tests

- Important group of non-parametric test
  - Non-parametric,
  - Exact $p$-values using a permutation null distribution.
  - No need for separate permutation distribution for each new dataset.
  - Permutation null distribution of rank tests only depends on sample size
  - Robust to outliers

---

# Ranks

Rank tests start from rank-transformed data.

- Let $Y_1, \ldots, Y_n$.
- In the absence of *ties*
  $$R_i=R(Y_i) = \#\{Y_j: Y_j\leq Y_i; j=1,\ldots, n\}$$
- Smallest observation has rank 1, second smallest rank 2, ... , largest observation gets rank $n$

```{r}
chol <- chol %>% 
  mutate(ranks = rank(cholest))
chol
```


Note, that we calculate the ranks for the pooled sample!

---

## Ties

Sometimes *ties* occur: two observations with identical values

```{r}
withTies <- c(403, 507, 507, 610, 651, 651, 651, 830, 900)
rank(withTies)
```

- Ties: 507 occurs twice, 651 occurs 3 times
- If ties occur *midranks* are used.

- **midrank** of observation $Y_i$ becomes
  \begin{eqnarray*}
   R_i &=& \frac{ \#\{Y_j: Y_j\leq Y_i\} + ( \#\{Y_j: Y_j < Y_i\} +1)}{2}.
   \end{eqnarray*}

---

# Wilcoxon-Mann-Whitney Test

 Simultaneously developed by Wilcoxon, and,  Mann and Whitney:  **Wilcoxon-Mann-Whitney**, **Wilcoxon rank sum test**  or **Mann-Whitney U test**

## Hypotheses

Under $H_0$ the distributions of the two groups are equal
$$H_0: f_1=f_2$$


Under the alternative $H_1$ the distributions differ in location $$H_1: \mu_1\neq \mu_2$$

$H_1$ assumes **location-shift**, we will relax this assumption later on.

## Test statistic

Classic T-test: assesses the difference in sample means $\bar{Y}_1-\bar{Y}_2$.

Here: Difference in sample means based on rank transformed data

Ranks based on the pooled sample (upon joining the observations from the two groups): $R_{ij}=R(Y_{ij})$ is de rank of observation $Y_{ij}$ in the pooled sample.

\[
  T = \frac{1}{n_1}\sum_{i=1}^{n_1} R(Y_{i1}) - \frac{1}{n_2}\sum_{i=1}^{n_2} R(Y_{i2}) .
\]

- Under $H_0$ we expect the average rank of the first group to be close to that of the second group so $T$ is close to zero.

- Under $H_1$ we expect the mean ranks to differ so that $T$ deviates from zero.

- It is sufficient to only calculate
  $$S_1=\sum_{i=1}^{n_1} R(Y_{i1})$$.

- $S_1$ is the sum of the ranks of the first group: *rank sum test*.

- This holds because
\[
  S_1+S_2 = \text{sum of all ranks} = 1+2+\cdots + n=\frac{1}{2}n(n+1).
\]


- $S_1$ (or $S_2$) is a good test statistic

- Use permutations to determine the exact permutation distribution. (Permute the ranks between the groups)

- For a given $n$ and no *ties* the rank transformed data is always
  $$1, 2, \ldots, n$$
- For given $n_1$ en $n_2$ the permutation distribution is always the same!
- With current computing power this is not so important any more.

---

## Standardized statistic

Often the standardized test statistic is used
\[
  T = \frac{S_1-\text{E}_{0}\left[S_1\right]}{\sqrt{\text{Var}_{0}\left[S_1\right]}},
\]

- with $\text{E}_{0}\left[S_1\right]$ and $\text{Var}_{0}\left[S_1\right]$ the expect mean and variance of S1 under $H_0$.

- Under $H_0$
 \[
   \text{E}_{0}\left[S_1\right]= \frac{1}{2}n_1(n+1) \;\;\;\;\text{ en }\;\;\;\; \text{Var}_{0}\left[S_1\right]=\frac{1}{12}n_1n_2(n+1).
 \]

- Under $H_0$ and when $\min(n_1,n_2)\rightarrow \infty$
 \[
    T = \frac{S_1-\text{E}_{0}\left[S_1\right]}{\sqrt{\text{Var}_{0}\left[S_1\right]}} \rightarrow N(0,1).
 \]

Asymptotically the standardised statistic follows a standard normal distribution!

---

## Cholesterol example

We illustrate the result for the cholesterol example using the R function `wilcox.test`.

```{r}
wilcox.test(cholest ~ group, data = chol)
```

- We reject $H_0$ ($p=$ `r format(wilcox.test(cholest~group,data=chol)$p.value,digits=2)` $<0.05$)

- The output shows $W=$ `r wilcox.test(cholest~group,data=chol)$statistic`?

- Lets calculate
```{r}
S1 <- chol %>% 
    mutate(ranks = rank(cholest)) %>%
    filter(group == 1) %>% 
    pull(ranks) %>%
    sum 
S1
S2 <- chol %>% 
    mutate(ranks = rank(cholest)) %>%
    filter(group == 2) %>% 
    pull(ranks) %>%
    sum 
S2
```

- Where does $W=$ `r wilcox.test(cholest~group,data=chol)$statistic` comes from?

---

## Mann and Whitney test

Mann and Whitney test in absence of ties:
\[
 U_1 = \sum_{i=1}^{n_1}\sum_{k=1}^{n_2} \text{I}\left\{Y_{i1}\geq Y_{k2}\right\}.
\]

- with $\text{I}\left\{.\right\}$ an indicator that equals 1  if the expression is true and is zero otherwise.

- U counts how many times an observation of the first group is larger or equal to an observation from the second group.

```{r}
y1 <- chol %>% 
    filter(group == 1) %>% 
    pull(cholest)

y2 <- chol %>% 
    filter(group == 2) %>% 
    pull(cholest)

u1Hlp <- sapply(y1, function(y1i, y2) {
  y1i >= y2
}, y2 = y2)
colnames(u1Hlp) <- y1
rownames(u1Hlp) <- y2
```

```{r}
u1Hlp
U1 <- sum(u1Hlp)
U1
```

It can be shown that $U_1 = S_1 - \frac{1}{2}n_1(n_1+1).$

```{r}
S1 - nGroups[1] * (nGroups[1] + 1) / 2
```

1. $U_1$ en $S_1$ contain the same information
2. $U_1$ is also a rank statistic, and
3. Exact test based on $U_1$ and $S_1$ are equivalent.

---

## Probabilistic index

- $U_1$ has a better interpretation feature
- Let $Y_j$ a random observation from group $j$ ($j=1,2$). Then
\begin{eqnarray*}
  \frac{1}{n_1n_2}\text{E}\left[U_1\right]
     &=& \text{P}\left[Y_1 \geq Y_2\right].
\end{eqnarray*}

So we can estimate the probability by calculating the mean of all indicator variable values $\text{I}\left\{Y_{i1}\geq Y_{k2}\right\}$. Note, that we did $n_1 \times n_2$ comparisons

```{r}
mean(u1Hlp)
U1 / (nGroups[1] * nGroups[2])
```

- Probability $\text{P}\left[Y_1 \geq Y_2\right]$ is referred to as the *probabilistic index*.
- It is the probability that a random observation of the first group is larger or equal than a random observation of the second group
- If $H_0$ holds $\text{P}\left[Y_1 \geq Y_2\right]=\frac{1}{2}$.

- R function `wilcox.test` does not return the Wilcoxon rank sum statistic. It returns the Mann-Whitney statistic $U_1$.
- Lets revisit the result
```{r}
wTest <- wilcox.test(cholest ~ group, data = chol)
wTest
U1
probInd <- wTest$statistic / prod(nGroups)
probInd
```

Because $p=$ `r format(wTest$p.value,digits=3)` $<0.05$ we conclude at the $5\%$ significance level that the median cholesterol level of hart patients is larger then that of healthy subjects.

  - Note that we have assumed that the location-shift model is valid in this conclusion.
  - We also know that higher cholesterol level are more likely for hart patients then for healthy subjects and this probability is
$U1/(n_1\times n_2)=$ `r probInd*100`%.
  - We should assess the location shift assumption. But this is not possible with only 5 observations.

Without the location-shift assumption the conclusion in terms of the probabilistic index remains valid!

  - So when we do not assume location shift we test for

\[H_0: F_1=F_2 \text{ vs } H_1: P[Y_1 \geq Y_2] \neq 0.5.\]


## Conclusion

There is a significant difference in the distribution of the cholesterol concentration of hart patients two days upon a stroke and that of healthy subject ($p=$ `r format(wTest$p.value,digits=3)`). It is more likely to observe higher cholesterol levels for hart patients then for healthy subjects. The point estimator for this probability is `r probInd*100`%.


# Comparison of $g$ groups

- Extend  $F$-test from a one-way ANOVA to non-parametric alternatives.

# DMH example

Assess genotoxicity of 1,2-dimethylhydrazine dihydrochloride (DMH)  (EU directive)

- 24 rats
- four groups with daily DMH dose
  - control
  - low
  - medium
  - high

- Genotoxicity in liver using comet assay on 150 liver cells per rat.
- Are there differences in DNA damage due to DMH dose?

## Comet Assay:

- Visualise DNA strand breaks
- Length comet tail is a proxy for strand breaks.

![Comet assay](https://raw.githubusercontent.com/GTPB/PSLS20/gh-pages/assets/figs/comet.jpg){ width=50% }


```{r}
dna <- read_delim("https://raw.githubusercontent.com/GTPB/PSLS20/master/data/dna.txt", delim = " ")
dna$dose <- as.factor(dna$dose)
dna
```


```{r}
dna %>%
  ggplot(aes(x = dose, y = length, fill = dose)) +
  geom_boxplot() +
  geom_point(position = "jitter")

dna %>%
  ggplot(aes(sample = length)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~dose)
```

- Strong indication that data in control group has a lower variance.
- 6 observations per group are too few to check the assumptions

```{r}
plot(lm(length ~ dose, data = dna))
```

# Kruskal-Wallis Rank Test

- The Kruskal-Wallis Rank Test (KW-test) is a  non-parameteric alternative  for ANOVA F-test.

-  Classical $F$-teststatistic can be written as
  \[
    F = \frac{\text{SST}/(g-1)}{\text{SSE}/(n-g)} = \frac{\text{SST}/(g-1)}{(\text{SSTot}-\text{SST})/(n-g)} ,
  \]
-  with $g$ the number of groups.

- SSTot depends only on  outcomes $\mathbf{y}$ and will not vary in permutation test.

- SST can be used as statistic
 $$\text{SST}=\sum_{j=1}^t n_j(\bar{Y}_j-\bar{Y})^2$$


-  The KW test statistic is based on SST on rank-transformed outcomes^[we assume that no *ties* are available],
  \[
     \text{SST} = \sum_{j=1}^g n_j \left(\bar{R}_j - \bar{R}\right)^2 = \sum_{j=1}^t n_j \left(\bar{R}_j - \frac{n+1}{2}\right)^2 ,
  \]
-  with $\bar{R}_j$ the mean of the ranks in group $j$, and $\bar{R}$ the mean of all ranks,
  \[
    \bar{R} = \frac{1}{n}(1+2+\cdots + n) = \frac{1}{n}\frac{1}{2}n(n+1) = \frac{n+1}{2}.
  \]
-  The KW teststatistic is given by
  \[
    KW = \frac{12}{n(n+1)}  \sum_{j=1}^g n_j \left(\bar{R}_j - \frac{n+1}{2}\right)^2.
  \]
-  The factor $\frac{12}{n(n+1)}$ is used so that $KW$ has a simple asymptotic null distribution. In particular under $H_0$, given thart $\min(n_1,\ldots, n_g)\rightarrow \infty$,
  \[
    KW  \rightarrow \chi^2_{t-1}.
  \]

-  The exact KW-test can be executed by calculating the permutation null distribution (that only depends on $n_1, \ldots, n_g$) to test
  $$H_0: f_1=\ldots=f_g \text{ vs } H_1: \text{ at least two means are different}.$$

- In order to allow $H_1$ to be formulated in terms of means, the assumption of locations shift should be valid.
- For DMH example this is not the case.
- If location-shift is invalid, we have to formulate $H_1$ in terms of probabilistic indices:
  $$H_0: f_1=\ldots=f_g \text{ vs } H_1: \exists\ j,k \in \{1,\ldots,g\} : \text{P}\left[Y_j\geq Y_k\right]\neq 0.5$$


## DNA Damage Example

```{r}
kruskal.test(length ~ dose, data = dna)
```

- On the $5\%$ level of significance we can reject the null hypothesis.

- R-functie `kruskal.test` only returns the asymptotic approximation for $p$-values.

- With only 6 observaties per groep, this is not a good approximation of the $p$-value

-  With the `coin` R package we can calculate the exacte $p$-value

```{r,warning=FALSE,message=FALSE}
library(coin)
kwPerm <- kruskal_test(length ~ dose,
  data = dna,
  distribution = approximate(B = 100000)
)
kwPerm
```

- We conclude that the difference in the distribution of the DNA damages due to the DMH dose is extremely significantly different.

- Posthoc analysis with WMW tests.

```{r}
pairwise.wilcox.test(dna$length, dna$dose)
```

- All DMH behandelingen are significantly different from the control.
- The DMH are not significantly different from one another.
- U1 does not occur in the `pairwise.wilcox.test` output. Point estimate on probability on higher DNA-damage?

```{r, echo=FALSE}
pairWilcox <- pairwise.wilcox.test(dna$length, dna$dose)
```

```{r}
nGroup <- table(dna$dose)
probInd <- combn(levels(dna$dose), 2, function(x) {
  test <- wilcox.test(length ~ dose, subset(dna, dose %in% x))
  return(test$statistic / prod(nGroup[x]))
})
names(probInd) <- combn(levels(dna$dose), 2, paste, collapse = "vs")
probInd
```

Because there are doubts on the location-shift model we draw our conclusions in terms of the probabilistic index.

### Conclusion

- There is an extremely significant difference in in the distribution of the DNA-damage measurements due to the treatment with DMH  ($p<0.001$ KW-test).
- DNA-damage is more likely upon DMH treatment than in the control treatment (all p=0.013, WMW-testen).
- The probability on higher DNA-damage upon exposure to DMH is 100% (Calculation of a CI on the probabilistic index is beyond the scope of the course)
- There are no significant differences in the distributions of the comit-lengths among the treatment with the different DMH concentrations ($p=$ `r paste(format(range(pairWilcox$p.value[,-1],na.rm=TRUE),digit=2),collapse="-")`).
- DMH shows already genotoxic effects at low dose.
- (All pairwise tests are corrected for multiple testing using Holm's method).

# Probabilistic index models (PIM)

- In the last decade we developed PIM models at Ghent University

- PIM generalises the Wilcoxon-Mann_Withney and Kruskal Wallis tests towards a regression framework for the probabilistic index. 

- PIM provides a nonparametric regression framework for experiments with simple and complex experimental designs. 

- Thas, O., Deneve, J., Clement, L. & Ottoy, J.P. (2012). Probabilistic Index Models. Journal of the Royal Society Series B (Statistical Methodology), 74(4): 623-671.  DOI: 10.1111/j.1467-9868.2011.01020.x

- De Neve, J. & Thas, O. (2015). A Regression Framework for Rank Tests Based on the Probabilistic Index Model, Journal of the American Statistical Association, 110:511, 1276-1283, DOI: 10.1080/01621459.2015.1016226


- Amorim G. et al. (2018). Small sample inference for probabilistic index models. Computational Statistics & Data Analysis, 121:137-148.  DOI: 10.1016/j.csda.2017.11.005

- De Schrijver, M. and De Neve, J. (2019) A tutorial on probabilistic index models: Regression models for the effect size P(Y1 < Y2). Psychological Methods, 24(4), 403–418. DOI: 10.1037/met0000194

- ...